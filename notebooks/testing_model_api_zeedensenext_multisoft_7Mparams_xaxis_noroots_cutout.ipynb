{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "testing_model_api_zeedensenext_multisoft_7Mparams_xaxis_noroots_cutout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DJYux3EMFOhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import time, math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiCclHjhFQ9P",
        "colab_type": "code",
        "outputId": "0c089d33-6896-428c-c111-7c59c215085c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/eva_research_team4/src/' #change dir to your project folder\n",
        "\n",
        "os.chdir('/content/' + root_path)\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5rEveGLmi6P",
        "colab_type": "code",
        "outputId": "bd3e0893-7797-4c61-f45e-c03847acdf03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!git branch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  augmentation\u001b[m\n",
            "  densenext\u001b[m\n",
            "  development\u001b[m\n",
            "  master\u001b[m\n",
            "  tfrecords_fix\u001b[m\n",
            "* \u001b[32mzeedense\u001b[m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTPh1jRp6UXQ",
        "colab_type": "code",
        "outputId": "15f2e707-1415-40f3-cadf-07bd97dcc386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "r0VIENINFOhV",
        "colab_type": "code",
        "outputId": "08792f00-bea6-4a48-cf5e-002baa53dfbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "#from all_models import DavidNet\n",
        "\n",
        "import run_util\n",
        "\n",
        "\n",
        "from importlib import reload\n",
        "reload(run_util)\n",
        "\n",
        "from run_util import Run\n",
        "\n",
        "import data_pipeline\n",
        "\n",
        "reload(data_pipeline)\n",
        "\n",
        "from zeedensenet import ZeeDenseNet\n",
        "\n",
        "from augmentation_utils import cutout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:56: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:139: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "32 conv0.1863308921420982 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.1863308921420982 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.1863308921420982 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DmYDmfSmFOhY",
        "colab_type": "code",
        "outputId": "edd60c10-34b8-48b1-a515-9d023198074e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "data_pipeline.get_data(dataset_name = \"CIFAR10\", tfrecords_flag=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving to tf records\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:28: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "Generating ../data/train/train.tfrecords\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/tfrecord_utils.py:26: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:41: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Generating ../data/eval/eval.tfrecords\n",
            "getting tf records complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0w2lxiFOhh",
        "colab_type": "text"
      },
      "source": [
        "######"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExKbiuvbFOhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "obj = Run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sK4c-9_7yLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mean = np.array([125.30691805, 122.95039414, 113.86538318])\n",
        "\n",
        "train_std= np.array([62.99321928, 62.08870764, 66.70489964])\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std)\n",
        "\n",
        "def data_aug_train(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    x = cutout(x, train_mean, prob=50)\n",
        "    \n",
        "    return (x, y)\n",
        "  \n",
        "def data_aug_test(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    #paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    #x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    #x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ06Buu372fX",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-alAs-eJPTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.95 #@param {type:\"number\"}\n",
        "\n",
        "MIN_MOMENTUM = 0.8 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 30 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "MIN_LEARNING_RATE = 0.000001 #@param {type:\"number\"}\n",
        "\n",
        "END_LR_SMOOTHING_PERC = 0.15 #@param {type:\"number\"}\n",
        "\n",
        "COMMENTS = \"Zeedensenet multisoftmax, cutout 7M without roots\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "params_tune = {\n",
        "    \n",
        "  \"epochs\":EPOCHS, \n",
        " \n",
        "  \"batch_size\" : BATCH_SIZE,\n",
        "\n",
        "  \"max_lr\": LEARNING_RATE,\n",
        "\n",
        "  \"min_lr\":MIN_LEARNING_RATE,\n",
        "\n",
        "  \"end_anneal_pc\":END_LR_SMOOTHING_PERC,\n",
        "\n",
        "  \"max_mom\":MOMENTUM,\n",
        " \n",
        "  \"min_mom\":MIN_MOMENTUM,\n",
        " \n",
        "  \"wd\":WEIGHT_DECAY,\n",
        "  \n",
        "  \"skip_testing_epochs\":0,\n",
        "    \n",
        "  \"batches_per_epoch\":50000//BATCH_SIZE,\n",
        "    \n",
        "  \"comments\":COMMENTS\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25iZ5W6fOK4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_tfrecs = data_pipeline.load_tfrecords(params_tune[\"batch_size\"])\n",
        "\n",
        "train_dataset = loaded_tfrecs[\"train\"]\n",
        "\n",
        "eval_dataset = loaded_tfrecs[\"eval\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6u8ypjZFOhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 10000\n",
        "\n",
        "    test_set = eval_dataset.map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 50000\n",
        "\n",
        "    train_set = train_dataset.map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GN0lpcbMFOhn",
        "colab_type": "code",
        "outputId": "2c6b42f0-9b4a-4a9f-b4ec-2126db36453f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "#import cProfile\n",
        "model2 = ZeeDenseNet(f_filter=64,  \n",
        "                     dimensions_dict= {\"dimensions_to_sample\":(8,8)}, \n",
        "                     gap_mode=\"x_axis\",\n",
        "                     layers_filters={0:64, 1:128},\n",
        "                    multisoft_list = [0, 1, 2],\n",
        "                     roots_flag=False\n",
        "                    )\n",
        "x = obj.run(params_tune, trn_data_supplier, tst_data_supplier, model = model2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 0.5362527917637248conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.7807041437470732conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.8671368033636291conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.5667235192508503conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "256 0.7063741718548029conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "256 0.9627151926819372conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "256 0.705389062032312conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "512 0.06560811918274123conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "512 0.3662194758852638conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "512 0.7774914671587909conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.533854671124858conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "128 0.5560113218737694conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbc979c5e2864c8e9ec5690bf86ba44e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 lr: 0.06666666666666667 train loss: 3.662 train acc:0.132 val loss: 3.232 val acc: 0.269 time:503.678\n",
            "epoch: 2 lr: 0.13333333333333333 train loss: 3.068 train acc:0.324 val loss: 2.872 val acc: 0.406 time:996.212\n",
            "epoch: 3 lr: 0.2 train loss: 2.545 train acc:0.465 val loss: 2.331 val acc: 0.549 time:1487.471\n",
            "epoch: 4 lr: 0.26666666666666666 train loss: 2.239 train acc:0.551 val loss: 1.973 val acc: 0.612 time:1978.835\n",
            "epoch: 5 lr: 0.3333333333333333 train loss: 2.017 train acc:0.622 val loss: 1.848 val acc: 0.665 time:2470.593\n",
            "epoch: 6 lr: 0.4 train loss: 1.898 train acc:0.657 val loss: 1.762 val acc: 0.684 time:2961.698\n",
            "epoch: 7 lr: 0.3810526315789474 train loss: 1.842 train acc:0.676 val loss: 1.752 val acc: 0.689 time:3452.867\n",
            "epoch: 8 lr: 0.36210526315789476 train loss: 1.845 train acc:0.675 val loss: 1.752 val acc: 0.689 time:3944.612\n",
            "epoch: 9 lr: 0.3431578947368421 train loss: 1.848 train acc:0.676 val loss: 1.752 val acc: 0.688 time:4436.037\n",
            "epoch: 10 lr: 0.3242105263157895 train loss: 1.840 train acc:0.675 val loss: 1.751 val acc: 0.689 time:4927.900\n",
            "epoch: 11 lr: 0.3052631578947369 train loss: 1.841 train acc:0.676 val loss: 1.753 val acc: 0.689 time:5420.544\n",
            "epoch: 12 lr: 0.28631578947368425 train loss: 1.841 train acc:0.677 val loss: 1.753 val acc: 0.688 time:5912.626\n",
            "epoch: 13 lr: 0.2673684210526316 train loss: 1.839 train acc:0.678 val loss: 1.751 val acc: 0.688 time:6404.730\n",
            "epoch: 14 lr: 0.248421052631579 train loss: 1.844 train acc:0.675 val loss: 1.752 val acc: 0.689 time:6895.601\n",
            "epoch: 15 lr: 0.22947368421052636 train loss: 1.841 train acc:0.677 val loss: 1.751 val acc: 0.688 time:7388.652\n",
            "epoch: 16 lr: 0.21052631578947373 train loss: 1.842 train acc:0.674 val loss: 1.753 val acc: 0.690 time:7881.099\n",
            "epoch: 17 lr: 0.1915789473684211 train loss: 1.844 train acc:0.675 val loss: 1.751 val acc: 0.689 time:8372.430\n",
            "epoch: 18 lr: 0.17263157894736847 train loss: 1.841 train acc:0.676 val loss: 1.752 val acc: 0.688 time:8864.163\n",
            "epoch: 19 lr: 0.15368421052631584 train loss: 1.846 train acc:0.676 val loss: 1.752 val acc: 0.689 time:9356.999\n",
            "epoch: 20 lr: 0.1347368421052632 train loss: 1.844 train acc:0.674 val loss: 1.752 val acc: 0.688 time:9848.006\n",
            "epoch: 21 lr: 0.11578947368421055 train loss: 1.846 train acc:0.675 val loss: 1.751 val acc: 0.689 time:10338.903\n",
            "epoch: 22 lr: 0.09684210526315795 train loss: 1.845 train acc:0.677 val loss: 1.751 val acc: 0.689 time:10831.276\n",
            "epoch: 23 lr: 0.07789473684210535 train loss: 1.842 train acc:0.677 val loss: 1.753 val acc: 0.689 time:11324.508\n",
            "epoch: 24 lr: 0.05894736842105269 train loss: 1.846 train acc:0.673 val loss: 1.751 val acc: 0.689 time:11816.117\n",
            "epoch: 25 lr: 0.04000000000000001 train loss: 1.844 train acc:0.676 val loss: 1.751 val acc: 0.689 time:12308.415\n",
            "epoch: 26 lr: 0.03333350000000001 train loss: 1.848 train acc:0.677 val loss: 1.753 val acc: 0.689 time:12800.735\n",
            "epoch: 27 lr: 0.026667000000000003 train loss: 1.841 train acc:0.674 val loss: 1.751 val acc: 0.689 time:13293.644\n",
            "epoch: 28 lr: 0.020000500000000004 train loss: 1.842 train acc:0.678 val loss: 1.751 val acc: 0.688 time:13785.831\n",
            "epoch: 29 lr: 0.013334000000000002 train loss: 1.844 train acc:0.675 val loss: 1.751 val acc: 0.689 time:14279.382\n",
            "epoch: 30 lr: 0.0066675 train loss: 1.842 train acc:0.675 val loss: 1.752 val acc: 0.690 time:14771.500\n",
            "\n",
            "   total_model_parameters  ...                                           comments\n",
            "0                 7840192  ...  Zeedensenet multisoftmax, cutout 7M without roots\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDxNAJ7RR29K",
        "colab_type": "code",
        "outputId": "061dc82e-3579-4a2b-9647-41f7305cdbbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "obj.model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"zee_dense_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_bn_rl_3 (ConvBnRl)      multiple                  1984      \n",
            "_________________________________________________________________\n",
            "res_blk (ResBlk)             multiple                  370176    \n",
            "_________________________________________________________________\n",
            "res_blk_1 (ResBlk)           multiple                  1477632   \n",
            "_________________________________________________________________\n",
            "res_blk_2 (ResBlk)           multiple                  5904384   \n",
            "_________________________________________________________________\n",
            "zee_conv_blk (ZeeConvBlk)    multiple                  79104     \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  6400      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  2560      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  3840      \n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f848f40e158b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                               \u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m                               \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                               print_fn=print_fn)\n\u001b[0m\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_summary\u001b[0;34m(model, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequential_like\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mprint_layer_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mprint_layer_summary_with_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_layer_summary\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mcls_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0mprint_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mcount_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1414\u001b[0m                          \u001b[0;34m', but the layer isn\\'t built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m                          \u001b[0;34m'You can build it manually via: `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m                          '.build(batch_input_shape)`.')\n\u001b[0m\u001b[1;32m   1417\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You tried to call `count_params` on dense_2, but the layer isn't built. You can build it manually via: `dense_2.build(batch_input_shape)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YiGsMVlzFOhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.show_missclassified_images(num_images=10, tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LAVqixK4FOhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.plot_confusion_matrix(tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4f0I3yOFOh0",
        "colab_type": "text"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUOqtPcHFOhw",
        "colab_type": "text"
      },
      "source": [
        "### LR Finder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K22g3QK0FOhx",
        "colab_type": "text"
      },
      "source": [
        "Creating a separate data supplier for lr finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUqw-X3yHMhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 100\n",
        "\n",
        "    test_set = eval_dataset.take(100).map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 1500\n",
        "\n",
        "    train_set = train_dataset.take(len_train).map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YyMzwcxPFOhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.lr_finder(model_fn, lr_list=[  0.004, 0.007, 0.01, 0.05, 0.09, 0.1, 0.4, 0.7, 0.9, 1], tst_data_supplier=tst_data_supplier_lr,\n",
        "             trn_data_supplier=trn_data_supplier_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uQeN4dhPFOh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model2 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model3 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model4 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model5 = ZeeDenseNet(f_filter=16, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "params_tune_grid = {\n",
        "    \n",
        "    \"model\": [model1, model2, model3, model4, model5, model6],\n",
        "    \n",
        "  \"epochs\":[30] ,\n",
        " \n",
        "  \"batch_size\" : [256],\n",
        "\n",
        "  \"max_lr\": [0.1, 0.4, 1, 10, 0.01],\n",
        "\n",
        "  \"min_lr\":[MIN_LEARNING_RATE],\n",
        "\n",
        "  \"end_anneal_pc\":[END_LR_SMOOTHING_PERC],\n",
        "\n",
        "  \"max_mom\":[MOMENTUM],\n",
        " \n",
        "  \"min_mom\":[MIN_MOMENTUM],\n",
        " \n",
        "  \"wd\":[WEIGHT_DECAY],\n",
        "  \n",
        "  \"skip_testing_epochs\":[0],\n",
        "    \n",
        "  \"batches_per_epoch\":[50000//BATCH_SIZE],\n",
        "    \n",
        "   \"comments\":[\"zeedense grid search\"]\n",
        "}\n",
        "\n",
        "\n",
        "obj.grid_search(params_tune_grid, trn_data_supplier, tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV1dIBh3FOh4",
        "colab_type": "text"
      },
      "source": [
        "##### Viewing log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JEYtCUgPFOh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"../data/run_logger.csv\").tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LNpxFwWFOh8",
        "colab_type": "text"
      },
      "source": [
        "#### Numpy implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgrIs7rpFOh9",
        "colab_type": "raw"
      },
      "source": [
        "x_train, y_train, x_test, y_test = data_pipeline.load_saved_numpy_data(\n",
        "    train_path=[\"../data/train/train_x.npy\",\"../data/train/train_y.npy\"],\n",
        "    test_path=[\"../data/test/test_x.npy\", \"../data/test/test_y.npy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ptU07KFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = x_test[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test[:30,:,:,:],\n",
        "                                                   y_test[:30])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global y_train\n",
        "    \n",
        "    global x_train\n",
        "    \n",
        "    len_test = x_train[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_train[:50,:,:,:], \n",
        "                                        y_train[:50])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCbZDfOFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier_lr(epoch, test_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_test\n",
        "  \n",
        "    global y_test\n",
        "  \n",
        "    sample_idx = np.random.choice(y_test.size, test_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_test_lr = x_test[sample_idx, :, :, :]\n",
        "\n",
        "    y_test_lr = y_test[sample_idx]\n",
        "    \n",
        "    len_test = x_test_lr[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test_lr,\n",
        "                                                   y_test_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch, train_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_train\n",
        "  \n",
        "    global y_train\n",
        "  \n",
        "    sample_idx = np.random.choice(y_train.size, train_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_train_lr = x_train[sample_idx, :, :, :]\n",
        "\n",
        "    y_train_lr = y_train[sample_idx]\n",
        "    \n",
        "    len_train = x_train_lr[0].shape[0]\n",
        "\n",
        "    train_set = tf.data.Dataset.from_tensor_slices((x_train_lr,\n",
        "                                                   y_train_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (train_set, len_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXJKLMH1XmnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
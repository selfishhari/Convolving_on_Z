{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "testing_model_api_zeedensenext_multisoft_7M_without_roots.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DJYux3EMFOhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import time, math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiCclHjhFQ9P",
        "colab_type": "code",
        "outputId": "cef0556f-b548-4d4c-8fbc-9c4af811f76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/eva_research_team4/src/' #change dir to your project folder\n",
        "\n",
        "os.chdir('/content/' + root_path)\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "r0VIENINFOhV",
        "colab_type": "code",
        "outputId": "68af73c3-0ebd-4648-ba7b-7bb37c26a5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#from all_models import DavidNet\n",
        "\n",
        "import run_util\n",
        "\n",
        "\n",
        "from importlib import reload\n",
        "reload(run_util)\n",
        "\n",
        "from run_util import Run\n",
        "\n",
        "import data_pipeline\n",
        "\n",
        "reload(data_pipeline)\n",
        "\n",
        "from zeedensenet import ZeeDenseNet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:56: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:139: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "32 conv0.9234744798103197 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.9234744798103197 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.9234744798103197 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DmYDmfSmFOhY",
        "colab_type": "code",
        "outputId": "f10f9d8d-d243-4f79-e8f6-de15f329f390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "data_pipeline.get_data(dataset_name = \"CIFAR10\", tfrecords_flag=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving to tf records\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:28: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "Generating ../data/train/train.tfrecords\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/tfrecord_utils.py:26: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:41: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Generating ../data/eval/eval.tfrecords\n",
            "getting tf records complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0w2lxiFOhh",
        "colab_type": "text"
      },
      "source": [
        "######"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExKbiuvbFOhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj = Run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sK4c-9_7yLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mean = np.array([125.30691805, 122.95039414, 113.86538318])\n",
        "\n",
        "train_std= np.array([62.99321928, 62.08870764, 66.70489964])\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std)\n",
        "\n",
        "def data_aug_train(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)\n",
        "  \n",
        "def data_aug_test(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    #paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    #x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    #x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ06Buu372fX",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-alAs-eJPTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.95 #@param {type:\"number\"}\n",
        "\n",
        "MIN_MOMENTUM = 0.8 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 30 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "MIN_LEARNING_RATE = 0.000001 #@param {type:\"number\"}\n",
        "\n",
        "END_LR_SMOOTHING_PERC = 0.15 #@param {type:\"number\"}\n",
        "\n",
        "COMMENTS = \"Zeedense 7M without roots\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "params_tune = {\n",
        "    \n",
        "  \"epochs\":EPOCHS, \n",
        " \n",
        "  \"batch_size\" : BATCH_SIZE,\n",
        "\n",
        "  \"max_lr\": LEARNING_RATE,\n",
        "\n",
        "  \"min_lr\":MIN_LEARNING_RATE,\n",
        "\n",
        "  \"end_anneal_pc\":END_LR_SMOOTHING_PERC,\n",
        "\n",
        "  \"max_mom\":MOMENTUM,\n",
        " \n",
        "  \"min_mom\":MIN_MOMENTUM,\n",
        " \n",
        "  \"wd\":WEIGHT_DECAY,\n",
        "  \n",
        "  \"skip_testing_epochs\":0,\n",
        "    \n",
        "  \"batches_per_epoch\":50000//BATCH_SIZE,\n",
        "    \n",
        "  \"comments\":COMMENTS\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25iZ5W6fOK4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_tfrecs = data_pipeline.load_tfrecords(params_tune[\"batch_size\"])\n",
        "\n",
        "train_dataset = loaded_tfrecs[\"train\"]\n",
        "\n",
        "eval_dataset = loaded_tfrecs[\"eval\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6u8ypjZFOhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 10000\n",
        "\n",
        "    test_set = eval_dataset.map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 50000\n",
        "\n",
        "    train_set = train_dataset.map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GN0lpcbMFOhn",
        "colab_type": "code",
        "outputId": "723ff5de-6bd9-4c6d-8520-f29a7a4d6eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#import cProfile\n",
        "model2 = ZeeDenseNet(f_filter=64,  \n",
        "                     dimensions_dict= {\"dimensions_to_sample\":(8,8)}, \n",
        "                     gap_mode=\"channel_axis\",\n",
        "                     layers_filters={0:64, 1:128},\n",
        "                    multisoft_list = [0, 1, 2],\n",
        "                     roots_flag=False\n",
        "                    )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 0.6559525772546261conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.5210953257742048conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.14132361755733103conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.8856064065988761conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "256 0.6007497925514657conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "256 0.11041561005868838conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "256 0.5354799961835454conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "512 0.8186291159605041conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "512 0.4554993663837419conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "512 0.9931725773844324conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.07480373309983601conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "128 0.10237239332063197conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aDnmTJ4_oaI",
        "colab_type": "code",
        "outputId": "194ac0f7-b00b-4b88-9dd5-638588095a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "x = obj.run(params_tune, trn_data_supplier, tst_data_supplier, model = model2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d8a01669d084f4285b75c5d338952a1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 lr: 0.06666666666666667 train loss: 2.478 train acc:0.457 val loss: 1.885 val acc: 0.570 time:580.434\n",
            "epoch: 2 lr: 0.13333333333333333 train loss: 1.490 train acc:0.695 val loss: 1.388 val acc: 0.712 time:1148.924\n",
            "epoch: 3 lr: 0.2 train loss: 1.119 train acc:0.792 val loss: 1.102 val acc: 0.786 time:1717.252\n",
            "epoch: 4 lr: 0.26666666666666666 train loss: 0.909 train acc:0.842 val loss: 0.885 val acc: 0.843 time:2285.412\n",
            "epoch: 5 lr: 0.3333333333333333 train loss: 0.736 train acc:0.887 val loss: 0.806 val acc: 0.861 time:2853.504\n",
            "epoch: 6 lr: 0.4 train loss: 0.624 train acc:0.913 val loss: 0.755 val acc: 0.875 time:3421.617\n",
            "epoch: 7 lr: 0.3810526315789474 train loss: 0.571 train acc:0.927 val loss: 0.743 val acc: 0.880 time:3989.155\n",
            "epoch: 8 lr: 0.36210526315789476 train loss: 0.571 train acc:0.926 val loss: 0.743 val acc: 0.880 time:4556.828\n",
            "epoch: 9 lr: 0.3431578947368421 train loss: 0.568 train acc:0.929 val loss: 0.744 val acc: 0.880 time:5125.094\n",
            "epoch: 10 lr: 0.3242105263157895 train loss: 0.574 train acc:0.927 val loss: 0.743 val acc: 0.880 time:5693.875\n",
            "epoch: 11 lr: 0.3052631578947369 train loss: 0.572 train acc:0.927 val loss: 0.744 val acc: 0.880 time:6263.378\n",
            "epoch: 12 lr: 0.28631578947368425 train loss: 0.571 train acc:0.927 val loss: 0.744 val acc: 0.880 time:6832.371\n",
            "epoch: 13 lr: 0.2673684210526316 train loss: 0.570 train acc:0.927 val loss: 0.743 val acc: 0.880 time:7400.828\n",
            "epoch: 14 lr: 0.248421052631579 train loss: 0.570 train acc:0.928 val loss: 0.743 val acc: 0.880 time:7967.982\n",
            "epoch: 15 lr: 0.22947368421052636 train loss: 0.570 train acc:0.927 val loss: 0.743 val acc: 0.880 time:8535.637\n",
            "epoch: 16 lr: 0.21052631578947373 train loss: 0.570 train acc:0.927 val loss: 0.743 val acc: 0.880 time:9103.967\n",
            "epoch: 17 lr: 0.1915789473684211 train loss: 0.569 train acc:0.928 val loss: 0.744 val acc: 0.880 time:9671.788\n",
            "epoch: 18 lr: 0.17263157894736847 train loss: 0.572 train acc:0.927 val loss: 0.743 val acc: 0.880 time:10240.464\n",
            "epoch: 19 lr: 0.15368421052631584 train loss: 0.570 train acc:0.927 val loss: 0.743 val acc: 0.880 time:10808.817\n",
            "epoch: 20 lr: 0.1347368421052632 train loss: 0.570 train acc:0.928 val loss: 0.743 val acc: 0.880 time:11377.750\n",
            "epoch: 21 lr: 0.11578947368421055 train loss: 0.570 train acc:0.926 val loss: 0.743 val acc: 0.880 time:11946.168\n",
            "epoch: 22 lr: 0.09684210526315795 train loss: 0.567 train acc:0.928 val loss: 0.743 val acc: 0.880 time:12514.465\n",
            "epoch: 23 lr: 0.07789473684210535 train loss: 0.572 train acc:0.928 val loss: 0.743 val acc: 0.880 time:13082.845\n",
            "epoch: 24 lr: 0.05894736842105269 train loss: 0.567 train acc:0.929 val loss: 0.743 val acc: 0.880 time:13651.018\n",
            "epoch: 25 lr: 0.04000000000000001 train loss: 0.572 train acc:0.927 val loss: 0.744 val acc: 0.880 time:14219.448\n",
            "epoch: 26 lr: 0.03333350000000001 train loss: 0.571 train acc:0.928 val loss: 0.743 val acc: 0.880 time:14789.132\n",
            "epoch: 27 lr: 0.026667000000000003 train loss: 0.569 train acc:0.928 val loss: 0.743 val acc: 0.880 time:15357.777\n",
            "epoch: 28 lr: 0.020000500000000004 train loss: 0.568 train acc:0.928 val loss: 0.742 val acc: 0.880 time:15925.995\n",
            "epoch: 29 lr: 0.013334000000000002 train loss: 0.569 train acc:0.927 val loss: 0.742 val acc: 0.880 time:16495.889\n",
            "epoch: 30 lr: 0.0066675 train loss: 0.567 train acc:0.928 val loss: 0.744 val acc: 0.880 time:17063.224\n",
            "\n",
            "   total_model_parameters  ...                   comments\n",
            "0                 7850432  ...  Zeedense 7M without roots\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDxNAJ7RR29K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YiGsMVlzFOhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.show_missclassified_images(num_images=10, tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LAVqixK4FOhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.plot_confusion_matrix(tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4f0I3yOFOh0",
        "colab_type": "text"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUOqtPcHFOhw",
        "colab_type": "text"
      },
      "source": [
        "### LR Finder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K22g3QK0FOhx",
        "colab_type": "text"
      },
      "source": [
        "Creating a separate data supplier for lr finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUqw-X3yHMhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 100\n",
        "\n",
        "    test_set = eval_dataset.take(100).map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 1500\n",
        "\n",
        "    train_set = train_dataset.take(len_train).map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YyMzwcxPFOhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.lr_finder(model_fn, lr_list=[  0.004, 0.007, 0.01, 0.05, 0.09, 0.1, 0.4, 0.7, 0.9, 1], tst_data_supplier=tst_data_supplier_lr,\n",
        "             trn_data_supplier=trn_data_supplier_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uQeN4dhPFOh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model2 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model3 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model4 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model5 = ZeeDenseNet(f_filter=16, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "params_tune_grid = {\n",
        "    \n",
        "    \"model\": [model1, model2, model3, model4, model5, model6],\n",
        "    \n",
        "  \"epochs\":[30] ,\n",
        " \n",
        "  \"batch_size\" : [256],\n",
        "\n",
        "  \"max_lr\": [0.1, 0.4, 1, 10, 0.01],\n",
        "\n",
        "  \"min_lr\":[MIN_LEARNING_RATE],\n",
        "\n",
        "  \"end_anneal_pc\":[END_LR_SMOOTHING_PERC],\n",
        "\n",
        "  \"max_mom\":[MOMENTUM],\n",
        " \n",
        "  \"min_mom\":[MIN_MOMENTUM],\n",
        " \n",
        "  \"wd\":[WEIGHT_DECAY],\n",
        "  \n",
        "  \"skip_testing_epochs\":[0],\n",
        "    \n",
        "  \"batches_per_epoch\":[50000//BATCH_SIZE],\n",
        "    \n",
        "   \"comments\":[\"zeedense grid search\"]\n",
        "}\n",
        "\n",
        "\n",
        "obj.grid_search(params_tune_grid, trn_data_supplier, tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV1dIBh3FOh4",
        "colab_type": "text"
      },
      "source": [
        "##### Viewing log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JEYtCUgPFOh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"../data/run_logger.csv\").tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LNpxFwWFOh8",
        "colab_type": "text"
      },
      "source": [
        "#### Numpy implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgrIs7rpFOh9",
        "colab_type": "raw"
      },
      "source": [
        "x_train, y_train, x_test, y_test = data_pipeline.load_saved_numpy_data(\n",
        "    train_path=[\"../data/train/train_x.npy\",\"../data/train/train_y.npy\"],\n",
        "    test_path=[\"../data/test/test_x.npy\", \"../data/test/test_y.npy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ptU07KFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = x_test[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test[:30,:,:,:],\n",
        "                                                   y_test[:30])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global y_train\n",
        "    \n",
        "    global x_train\n",
        "    \n",
        "    len_test = x_train[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_train[:50,:,:,:], \n",
        "                                        y_train[:50])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCbZDfOFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier_lr(epoch, test_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_test\n",
        "  \n",
        "    global y_test\n",
        "  \n",
        "    sample_idx = np.random.choice(y_test.size, test_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_test_lr = x_test[sample_idx, :, :, :]\n",
        "\n",
        "    y_test_lr = y_test[sample_idx]\n",
        "    \n",
        "    len_test = x_test_lr[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test_lr,\n",
        "                                                   y_test_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch, train_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_train\n",
        "  \n",
        "    global y_train\n",
        "  \n",
        "    sample_idx = np.random.choice(y_train.size, train_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_train_lr = x_train[sample_idx, :, :, :]\n",
        "\n",
        "    y_train_lr = y_train[sample_idx]\n",
        "    \n",
        "    len_train = x_train_lr[0].shape[0]\n",
        "\n",
        "    train_set = tf.data.Dataset.from_tensor_slices((x_train_lr,\n",
        "                                                   y_train_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (train_set, len_train)"
      ]
    }
  ]
}
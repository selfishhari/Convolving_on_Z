{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "testing_model_api_zeedensenext_multisoft_501kparams_xaxis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DJYux3EMFOhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import time, math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiCclHjhFQ9P",
        "colab_type": "code",
        "outputId": "4028fc90-051d-4668-b0f2-28816726db1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/eva_research_team4/src/' #change dir to your project folder\n",
        "\n",
        "os.chdir('/content/' + root_path)\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5rEveGLmi6P",
        "colab_type": "code",
        "outputId": "ca0dada7-b656-4927-c27d-1933ea0e2843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git branch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  augmentation\u001b[m\n",
            "  densenext\u001b[m\n",
            "  development\u001b[m\n",
            "  master\u001b[m\n",
            "  tfrecords_fix\u001b[m\n",
            "* \u001b[32mzeedense\u001b[m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTPh1jRp6UXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ca6e367c-cfb3-431d-9c96-e98deef8b8a5"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects:  25% (1/4)\u001b[K\rremote: Counting objects:  50% (2/4)\u001b[K\rremote: Counting objects:  75% (3/4)\u001b[K\rremote: Counting objects: 100% (4/4)\u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)\u001b[K\rremote: Compressing objects:  66% (2/3)\u001b[K\rremote: Compressing objects: 100% (3/3)\u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 1), reused 2 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)   \rUnpacking objects:  50% (2/4)   \rUnpacking objects:  75% (3/4)   \rUnpacking objects: 100% (4/4)   \rUnpacking objects: 100% (4/4), done.\n",
            "From https://github.com/selfishhari/eva_research_team4\n",
            "   1121542..453dc2c  zeedense   -> origin/zeedense\n",
            "Updating 1121542..453dc2c\n",
            "Fast-forward\n",
            " src/all_models.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "r0VIENINFOhV",
        "colab_type": "code",
        "outputId": "a2ace643-ade7-4797-b0ea-679a0fc192fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#from all_models import DavidNet\n",
        "\n",
        "import run_util\n",
        "\n",
        "\n",
        "from importlib import reload\n",
        "reload(run_util)\n",
        "\n",
        "from run_util import Run\n",
        "\n",
        "import data_pipeline\n",
        "\n",
        "reload(data_pipeline)\n",
        "\n",
        "from zeedensenet import ZeeDenseNet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:56: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:139: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "32 conv0.6289714457296209 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.6289714457296209 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.6289714457296209 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DmYDmfSmFOhY",
        "colab_type": "code",
        "outputId": "287acb8a-b584-4716-a9f8-54b996af3e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "data_pipeline.get_data(dataset_name = \"CIFAR10\", tfrecords_flag=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving to tf records\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:28: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "Generating ../data/train/train.tfrecords\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/tfrecord_utils.py:26: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:41: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Generating ../data/eval/eval.tfrecords\n",
            "getting tf records complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0w2lxiFOhh",
        "colab_type": "text"
      },
      "source": [
        "######"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExKbiuvbFOhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "obj = Run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sK4c-9_7yLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mean = np.array([125.30691805, 122.95039414, 113.86538318])\n",
        "\n",
        "train_std= np.array([62.99321928, 62.08870764, 66.70489964])\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std)\n",
        "\n",
        "def data_aug_train(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)\n",
        "  \n",
        "def data_aug_test(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    #paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    #x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    #x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ06Buu372fX",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-alAs-eJPTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.95 #@param {type:\"number\"}\n",
        "\n",
        "MIN_MOMENTUM = 0.8 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 30 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "MIN_LEARNING_RATE = 0.000001 #@param {type:\"number\"}\n",
        "\n",
        "END_LR_SMOOTHING_PERC = 0.15 #@param {type:\"number\"}\n",
        "\n",
        "COMMENTS = \"Zeedensenet multisoftmax, gap on channels\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "params_tune = {\n",
        "    \n",
        "  \"epochs\":EPOCHS, \n",
        " \n",
        "  \"batch_size\" : BATCH_SIZE,\n",
        "\n",
        "  \"max_lr\": LEARNING_RATE,\n",
        "\n",
        "  \"min_lr\":MIN_LEARNING_RATE,\n",
        "\n",
        "  \"end_anneal_pc\":END_LR_SMOOTHING_PERC,\n",
        "\n",
        "  \"max_mom\":MOMENTUM,\n",
        " \n",
        "  \"min_mom\":MIN_MOMENTUM,\n",
        " \n",
        "  \"wd\":WEIGHT_DECAY,\n",
        "  \n",
        "  \"skip_testing_epochs\":0,\n",
        "    \n",
        "  \"batches_per_epoch\":50000//BATCH_SIZE,\n",
        "    \n",
        "  \"comments\":COMMENTS\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25iZ5W6fOK4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_tfrecs = data_pipeline.load_tfrecords(params_tune[\"batch_size\"])\n",
        "\n",
        "train_dataset = loaded_tfrecs[\"train\"]\n",
        "\n",
        "eval_dataset = loaded_tfrecs[\"eval\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6u8ypjZFOhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 10000\n",
        "\n",
        "    test_set = eval_dataset.map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 50000\n",
        "\n",
        "    train_set = train_dataset.map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GN0lpcbMFOhn",
        "colab_type": "code",
        "outputId": "39710b63-1a4e-4d24-a4d8-98e90ea77e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "#import cProfile\n",
        "model2 = ZeeDenseNet(f_filter=16,  \n",
        "                     dimensions_dict= {\"dimensions_to_sample\":(8,8)}, \n",
        "                     gap_mode=\"x_axis\",\n",
        "                     layers_filters={0:8, 1:16},\n",
        "                    multisoft_list = [0, 1, 2])\n",
        "x = obj.run(params_tune, trn_data_supplier, tst_data_supplier, model = model2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 0.1079321950659844conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 0.8734570080900664conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 0.5839776632998335conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 0.8446381365764188conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.3414452505507284conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.8368188800739127conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.19561978035359084conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.3475704783202016conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.369909154831797conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.7729274501019461conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "8 0.04696057248297536conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.863491398921231conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.7407718588721659conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.8882867762004197conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.6611157375418562conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.32715322121846835conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.33401166345786815conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.4067401864974991conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.06938128908496433conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.17325924331192066conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.5113727994430001conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.7709963423464425conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.36116109508399363conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.676574833876222conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.5374151914766531conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.2727567921749078conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2f3fe81a5264dedae5847b69278c4f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 lr: 0.06666666666666667 train loss: 3.313 train acc:0.262 val loss: 2.703 val acc: 0.393 time:163.055\n",
            "epoch: 2 lr: 0.13333333333333333 train loss: 2.492 train acc:0.453 val loss: 2.343 val acc: 0.442 time:320.694\n",
            "epoch: 3 lr: 0.2 train loss: 2.129 train acc:0.572 val loss: 1.987 val acc: 0.564 time:478.408\n",
            "epoch: 4 lr: 0.26666666666666666 train loss: 1.830 train acc:0.649 val loss: 2.035 val acc: 0.553 time:635.433\n",
            "epoch: 5 lr: 0.3333333333333333 train loss: 1.651 train acc:0.698 val loss: 1.868 val acc: 0.622 time:792.036\n",
            "epoch: 6 lr: 0.4 train loss: 1.474 train acc:0.739 val loss: 1.584 val acc: 0.721 time:948.675\n",
            "epoch: 7 lr: 0.3810526315789474 train loss: 1.321 train acc:0.773 val loss: 1.545 val acc: 0.689 time:1105.220\n",
            "epoch: 8 lr: 0.36210526315789476 train loss: 1.228 train acc:0.793 val loss: 1.574 val acc: 0.736 time:1262.409\n",
            "epoch: 9 lr: 0.3431578947368421 train loss: 1.156 train acc:0.809 val loss: 1.578 val acc: 0.735 time:1420.257\n",
            "epoch: 10 lr: 0.3242105263157895 train loss: 1.095 train acc:0.823 val loss: 1.541 val acc: 0.738 time:1577.802\n",
            "epoch: 11 lr: 0.3052631578947369 train loss: 1.042 train acc:0.835 val loss: 1.392 val acc: 0.761 time:1735.218\n",
            "epoch: 12 lr: 0.28631578947368425 train loss: 1.004 train acc:0.844 val loss: 1.527 val acc: 0.756 time:1893.112\n",
            "epoch: 13 lr: 0.2673684210526316 train loss: 0.967 train acc:0.854 val loss: 1.311 val acc: 0.783 time:2050.609\n",
            "epoch: 14 lr: 0.248421052631579 train loss: 0.931 train acc:0.864 val loss: 1.362 val acc: 0.757 time:2208.039\n",
            "epoch: 15 lr: 0.22947368421052636 train loss: 0.895 train acc:0.871 val loss: 1.173 val acc: 0.824 time:2365.310\n",
            "epoch: 16 lr: 0.21052631578947373 train loss: 0.871 train acc:0.877 val loss: 1.163 val acc: 0.826 time:2522.992\n",
            "epoch: 17 lr: 0.1915789473684211 train loss: 0.847 train acc:0.884 val loss: 1.183 val acc: 0.824 time:2680.405\n",
            "epoch: 18 lr: 0.17263157894736847 train loss: 0.820 train acc:0.890 val loss: 1.112 val acc: 0.839 time:2837.810\n",
            "epoch: 19 lr: 0.15368421052631584 train loss: 0.795 train acc:0.895 val loss: 1.057 val acc: 0.845 time:2995.561\n",
            "epoch: 20 lr: 0.1347368421052632 train loss: 0.782 train acc:0.901 val loss: 1.094 val acc: 0.848 time:3153.465\n",
            "epoch: 21 lr: 0.11578947368421055 train loss: 0.773 train acc:0.905 val loss: 1.096 val acc: 0.848 time:3310.193\n",
            "epoch: 22 lr: 0.09684210526315795 train loss: 0.766 train acc:0.907 val loss: 1.067 val acc: 0.851 time:3468.045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDxNAJ7RR29K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "5dfd0f3b-ab36-4dee-fdff-353937f41292"
      },
      "source": [
        "obj.model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f848f40e158b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'obj' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YiGsMVlzFOhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.show_missclassified_images(num_images=10, tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LAVqixK4FOhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.plot_confusion_matrix(tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4f0I3yOFOh0",
        "colab_type": "text"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUOqtPcHFOhw",
        "colab_type": "text"
      },
      "source": [
        "### LR Finder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K22g3QK0FOhx",
        "colab_type": "text"
      },
      "source": [
        "Creating a separate data supplier for lr finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUqw-X3yHMhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 100\n",
        "\n",
        "    test_set = eval_dataset.take(100).map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 1500\n",
        "\n",
        "    train_set = train_dataset.take(len_train).map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YyMzwcxPFOhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.lr_finder(model_fn, lr_list=[  0.004, 0.007, 0.01, 0.05, 0.09, 0.1, 0.4, 0.7, 0.9, 1], tst_data_supplier=tst_data_supplier_lr,\n",
        "             trn_data_supplier=trn_data_supplier_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uQeN4dhPFOh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model2 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model3 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model4 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model5 = ZeeDenseNet(f_filter=16, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "params_tune_grid = {\n",
        "    \n",
        "    \"model\": [model1, model2, model3, model4, model5, model6],\n",
        "    \n",
        "  \"epochs\":[30] ,\n",
        " \n",
        "  \"batch_size\" : [256],\n",
        "\n",
        "  \"max_lr\": [0.1, 0.4, 1, 10, 0.01],\n",
        "\n",
        "  \"min_lr\":[MIN_LEARNING_RATE],\n",
        "\n",
        "  \"end_anneal_pc\":[END_LR_SMOOTHING_PERC],\n",
        "\n",
        "  \"max_mom\":[MOMENTUM],\n",
        " \n",
        "  \"min_mom\":[MIN_MOMENTUM],\n",
        " \n",
        "  \"wd\":[WEIGHT_DECAY],\n",
        "  \n",
        "  \"skip_testing_epochs\":[0],\n",
        "    \n",
        "  \"batches_per_epoch\":[50000//BATCH_SIZE],\n",
        "    \n",
        "   \"comments\":[\"zeedense grid search\"]\n",
        "}\n",
        "\n",
        "\n",
        "obj.grid_search(params_tune_grid, trn_data_supplier, tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV1dIBh3FOh4",
        "colab_type": "text"
      },
      "source": [
        "##### Viewing log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JEYtCUgPFOh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"../data/run_logger.csv\").tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LNpxFwWFOh8",
        "colab_type": "text"
      },
      "source": [
        "#### Numpy implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgrIs7rpFOh9",
        "colab_type": "raw"
      },
      "source": [
        "x_train, y_train, x_test, y_test = data_pipeline.load_saved_numpy_data(\n",
        "    train_path=[\"../data/train/train_x.npy\",\"../data/train/train_y.npy\"],\n",
        "    test_path=[\"../data/test/test_x.npy\", \"../data/test/test_y.npy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ptU07KFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = x_test[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test[:30,:,:,:],\n",
        "                                                   y_test[:30])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global y_train\n",
        "    \n",
        "    global x_train\n",
        "    \n",
        "    len_test = x_train[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_train[:50,:,:,:], \n",
        "                                        y_train[:50])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCbZDfOFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier_lr(epoch, test_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_test\n",
        "  \n",
        "    global y_test\n",
        "  \n",
        "    sample_idx = np.random.choice(y_test.size, test_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_test_lr = x_test[sample_idx, :, :, :]\n",
        "\n",
        "    y_test_lr = y_test[sample_idx]\n",
        "    \n",
        "    len_test = x_test_lr[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test_lr,\n",
        "                                                   y_test_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch, train_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_train\n",
        "  \n",
        "    global y_train\n",
        "  \n",
        "    sample_idx = np.random.choice(y_train.size, train_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_train_lr = x_train[sample_idx, :, :, :]\n",
        "\n",
        "    y_train_lr = y_train[sample_idx]\n",
        "    \n",
        "    len_train = x_train_lr[0].shape[0]\n",
        "\n",
        "    train_set = tf.data.Dataset.from_tensor_slices((x_train_lr,\n",
        "                                                   y_train_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (train_set, len_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXJKLMH1XmnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
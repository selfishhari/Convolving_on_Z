{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "testing_model_api_zeedensenext_multisoft_501kparams.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DJYux3EMFOhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import time, math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiCclHjhFQ9P",
        "colab_type": "code",
        "outputId": "fd9af989-e543-415a-83be-b1aa1c98b568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/eva_research_team4/src/' #change dir to your project folder\n",
        "\n",
        "os.chdir('/content/' + root_path)\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5rEveGLmi6P",
        "colab_type": "code",
        "outputId": "385e0ebe-f94f-45c1-eef1-1eaf2992c98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git branch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  augmentation\u001b[m\n",
            "  densenext\u001b[m\n",
            "  development\u001b[m\n",
            "  master\u001b[m\n",
            "  tfrecords_fix\u001b[m\n",
            "* \u001b[32mzeedense\u001b[m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTPh1jRp6UXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "463ed82a-04d6-4d30-95b2-a1dd346554af"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "r0VIENINFOhV",
        "colab_type": "code",
        "outputId": "3f4dec53-5974-4c9c-868b-198ea46ee200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#from all_models import DavidNet\n",
        "\n",
        "import run_util\n",
        "\n",
        "\n",
        "from importlib import reload\n",
        "reload(run_util)\n",
        "\n",
        "from run_util import Run\n",
        "\n",
        "import data_pipeline\n",
        "\n",
        "reload(data_pipeline)\n",
        "\n",
        "from zeedensenet import ZeeDenseNet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:54: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:56: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/run_util.py:139: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "32 conv0.27260135918100314 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.27260135918100314 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 conv0.27260135918100314 (3, 3) (1, 1) same (1, 1) None glorot_uniform\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DmYDmfSmFOhY",
        "colab_type": "code",
        "outputId": "4bafb40c-725c-4978-bc95-d4bf4a305bce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "data_pipeline.get_data(dataset_name = \"CIFAR10\", tfrecords_flag=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving to tf records\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:28: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
            "Generating ../data/train/train.tfrecords\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/tfrecord_utils.py:26: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/Colab Notebooks/eva_research_team4/src/data_pipeline.py:41: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Generating ../data/eval/eval.tfrecords\n",
            "getting tf records complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0w2lxiFOhh",
        "colab_type": "text"
      },
      "source": [
        "######"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExKbiuvbFOhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "obj = Run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sK4c-9_7yLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mean = np.array([125.30691805, 122.95039414, 113.86538318])\n",
        "\n",
        "train_std= np.array([62.99321928, 62.08870764, 66.70489964])\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std)\n",
        "\n",
        "def data_aug_train(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)\n",
        "  \n",
        "def data_aug_test(x, y):\n",
        "    \n",
        "    #x = tf.image.per_image_standardization(x)\n",
        "    \n",
        "    x = normalize(x)\n",
        "        \n",
        "    #x = tf.image.random_flip_left_right(x)\n",
        "    \n",
        "    #paddings = [(4, 4), (4, 4), (0, 0)]\n",
        "    \n",
        "    #x = tf.pad(x, paddings, \"REFLECT\")\n",
        "    \n",
        "    #x = tf.random_crop(x, [32, 32, 3])\n",
        "    \n",
        "    return (x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ06Buu372fX",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-alAs-eJPTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.95 #@param {type:\"number\"}\n",
        "\n",
        "MIN_MOMENTUM = 0.8 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 30 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "MIN_LEARNING_RATE = 0.000001 #@param {type:\"number\"}\n",
        "\n",
        "END_LR_SMOOTHING_PERC = 0.15 #@param {type:\"number\"}\n",
        "\n",
        "COMMENTS = \"Zeedensenet multisoftmax, gap on channels\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "params_tune = {\n",
        "    \n",
        "  \"epochs\":EPOCHS, \n",
        " \n",
        "  \"batch_size\" : BATCH_SIZE,\n",
        "\n",
        "  \"max_lr\": LEARNING_RATE,\n",
        "\n",
        "  \"min_lr\":MIN_LEARNING_RATE,\n",
        "\n",
        "  \"end_anneal_pc\":END_LR_SMOOTHING_PERC,\n",
        "\n",
        "  \"max_mom\":MOMENTUM,\n",
        " \n",
        "  \"min_mom\":MIN_MOMENTUM,\n",
        " \n",
        "  \"wd\":WEIGHT_DECAY,\n",
        "  \n",
        "  \"skip_testing_epochs\":0,\n",
        "    \n",
        "  \"batches_per_epoch\":50000//BATCH_SIZE,\n",
        "    \n",
        "  \"comments\":COMMENTS\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25iZ5W6fOK4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_tfrecs = data_pipeline.load_tfrecords(params_tune[\"batch_size\"])\n",
        "\n",
        "train_dataset = loaded_tfrecs[\"train\"]\n",
        "\n",
        "eval_dataset = loaded_tfrecs[\"eval\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6u8ypjZFOhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 10000\n",
        "\n",
        "    test_set = eval_dataset.map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 50000\n",
        "\n",
        "    train_set = train_dataset.map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GN0lpcbMFOhn",
        "colab_type": "code",
        "outputId": "86407284-cee5-404b-896d-1795edc8ffe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#import cProfile\n",
        "model2 = ZeeDenseNet(f_filter=16,  \n",
        "                     dimensions_dict= {\"dimensions_to_sample\":(8,8)}, \n",
        "                     gap_mode=\"channel_axis\",\n",
        "                     layers_filters={0:8, 1:16},\n",
        "                    multisoft_list = [0, 1, 2])\n",
        "x = obj.run(params_tune, trn_data_supplier, tst_data_supplier, model = model2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 0.8031016948496695conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 0.7991130454577566conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 0.4917460292034307conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "32 0.5403579815635616conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.4777684201780009conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.6103264373579538conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "64 0.8500714806728025conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.6611443193700529conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.5313123336669981conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "128 0.48477475624467947conv (3, 3) (1, 1) same (1, 1) None glorot_uniform\n",
            "8 0.044375522254821886conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.3259446518313355conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.9073061754711111conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.07713148702073569conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.39993281881552756conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.6624991145314976conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.9215607814195604conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "8 0.1626071701160563conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.8205461084785309conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.7055889827586826conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.5992114706843521conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.638880523094749conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.9821083585491365conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.7259199608270566conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.8741173961440996conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n",
            "16 0.08469560694829947conv (3, 3) (1, 1) same (2, 2) None glorot_uniform\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "001e8adbcd514833880f4575cd88c95f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 lr: 0.06666666666666667 train loss: 3.288 train acc:0.261 val loss: 2.665 val acc: 0.417 time:183.967\n",
            "epoch: 2 lr: 0.13333333333333333 train loss: 2.356 train acc:0.501 val loss: 2.162 val acc: 0.523 time:361.764\n",
            "epoch: 3 lr: 0.2 train loss: 1.845 train acc:0.628 val loss: 2.211 val acc: 0.569 time:539.822\n",
            "epoch: 4 lr: 0.26666666666666666 train loss: 1.596 train acc:0.691 val loss: 1.560 val acc: 0.705 time:718.380\n",
            "epoch: 5 lr: 0.3333333333333333 train loss: 1.464 train acc:0.730 val loss: 1.518 val acc: 0.700 time:896.464\n",
            "epoch: 6 lr: 0.4 train loss: 1.336 train acc:0.766 val loss: 1.313 val acc: 0.747 time:1074.915\n",
            "epoch: 7 lr: 0.3810526315789474 train loss: 1.223 train acc:0.792 val loss: 1.337 val acc: 0.756 time:1253.057\n",
            "epoch: 8 lr: 0.36210526315789476 train loss: 1.156 train acc:0.809 val loss: 1.244 val acc: 0.792 time:1431.484\n",
            "epoch: 9 lr: 0.3431578947368421 train loss: 1.088 train acc:0.825 val loss: 1.414 val acc: 0.780 time:1609.085\n",
            "epoch: 10 lr: 0.3242105263157895 train loss: 1.035 train acc:0.838 val loss: 1.249 val acc: 0.802 time:1787.050\n",
            "epoch: 11 lr: 0.3052631578947369 train loss: 0.995 train acc:0.849 val loss: 1.160 val acc: 0.810 time:1964.113\n",
            "epoch: 12 lr: 0.28631578947368425 train loss: 0.964 train acc:0.858 val loss: 1.281 val acc: 0.808 time:2141.217\n",
            "epoch: 13 lr: 0.2673684210526316 train loss: 0.928 train acc:0.867 val loss: 1.141 val acc: 0.812 time:2317.796\n",
            "epoch: 14 lr: 0.248421052631579 train loss: 0.893 train acc:0.877 val loss: 1.179 val acc: 0.810 time:2494.920\n",
            "epoch: 15 lr: 0.22947368421052636 train loss: 0.868 train acc:0.883 val loss: 1.234 val acc: 0.813 time:2671.787\n",
            "epoch: 16 lr: 0.21052631578947373 train loss: 0.840 train acc:0.892 val loss: 1.210 val acc: 0.821 time:2848.203\n",
            "epoch: 17 lr: 0.1915789473684211 train loss: 0.813 train acc:0.900 val loss: 1.101 val acc: 0.833 time:3024.058\n",
            "epoch: 18 lr: 0.17263157894736847 train loss: 0.786 train acc:0.906 val loss: 1.064 val acc: 0.843 time:3200.795\n",
            "epoch: 19 lr: 0.15368421052631584 train loss: 0.767 train acc:0.915 val loss: 1.029 val acc: 0.850 time:3377.903\n",
            "epoch: 20 lr: 0.1347368421052632 train loss: 0.745 train acc:0.918 val loss: 1.042 val acc: 0.852 time:3555.552\n",
            "epoch: 21 lr: 0.11578947368421055 train loss: 0.735 train acc:0.922 val loss: 1.023 val acc: 0.860 time:3732.256\n",
            "epoch: 22 lr: 0.09684210526315795 train loss: 0.728 train acc:0.923 val loss: 0.987 val acc: 0.861 time:3909.733\n",
            "epoch: 23 lr: 0.07789473684210535 train loss: 0.717 train acc:0.928 val loss: 0.971 val acc: 0.863 time:4087.673\n",
            "epoch: 24 lr: 0.05894736842105269 train loss: 0.709 train acc:0.930 val loss: 0.961 val acc: 0.867 time:4265.833\n",
            "epoch: 25 lr: 0.04000000000000001 train loss: 0.699 train acc:0.932 val loss: 0.965 val acc: 0.866 time:4443.245\n",
            "epoch: 26 lr: 0.03333350000000001 train loss: 0.696 train acc:0.935 val loss: 0.963 val acc: 0.866 time:4621.469\n",
            "epoch: 27 lr: 0.026667000000000003 train loss: 0.698 train acc:0.933 val loss: 0.964 val acc: 0.866 time:4799.390\n",
            "epoch: 28 lr: 0.020000500000000004 train loss: 0.696 train acc:0.933 val loss: 0.964 val acc: 0.866 time:4976.837\n",
            "epoch: 29 lr: 0.013334000000000002 train loss: 0.696 train acc:0.933 val loss: 0.965 val acc: 0.867 time:5154.005\n",
            "epoch: 30 lr: 0.0066675 train loss: 0.695 train acc:0.933 val loss: 0.964 val acc: 0.866 time:5331.370\n",
            "\n",
            "   total_model_parameters  ...                                   comments\n",
            "0                  501584  ...  Zeedensenet multisoftmax, gap on channels\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDxNAJ7RR29K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "b1c14268-5637-4dea-8be5-2a1b9ff03fa5"
      },
      "source": [
        "obj.model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"zee_dense_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_bn_rl_3 (ConvBnRl)      multiple                  496       \n",
            "_________________________________________________________________\n",
            "res_blk (ResBlk)             multiple                  23424     \n",
            "_________________________________________________________________\n",
            "res_blk_1 (ResBlk)           multiple                  92928     \n",
            "_________________________________________________________________\n",
            "res_blk_2 (ResBlk)           multiple                  370176    \n",
            "_________________________________________________________________\n",
            "zee_conv_blk (ZeeConvBlk)    multiple                  10560     \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  3520      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  640       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  1600      \n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f848f40e158b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                               \u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m                               \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                               print_fn=print_fn)\n\u001b[0m\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_summary\u001b[0;34m(model, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequential_like\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mprint_layer_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mprint_layer_summary_with_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_layer_summary\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mcls_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0mprint_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mcount_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1414\u001b[0m                          \u001b[0;34m', but the layer isn\\'t built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m                          \u001b[0;34m'You can build it manually via: `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m                          '.build(batch_input_shape)`.')\n\u001b[0m\u001b[1;32m   1417\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You tried to call `count_params` on dense_2, but the layer isn't built. You can build it manually via: `dense_2.build(batch_input_shape)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YiGsMVlzFOhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.show_missclassified_images(num_images=10, tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LAVqixK4FOhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.plot_confusion_matrix(tst_data_supplier=tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4f0I3yOFOh0",
        "colab_type": "text"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUOqtPcHFOhw",
        "colab_type": "text"
      },
      "source": [
        "### LR Finder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K22g3QK0FOhx",
        "colab_type": "text"
      },
      "source": [
        "Creating a separate data supplier for lr finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUqw-X3yHMhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tst_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = 100\n",
        "\n",
        "    test_set = eval_dataset.take(100).map(data_aug_test).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global train_dataset\n",
        "    \n",
        "    len_train = 1500\n",
        "\n",
        "    train_set = train_dataset.take(len_train).map(data_aug_train).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (train_set, len_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YyMzwcxPFOhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj.lr_finder(model_fn, lr_list=[  0.004, 0.007, 0.01, 0.05, 0.09, 0.1, 0.4, 0.7, 0.9, 1], tst_data_supplier=tst_data_supplier_lr,\n",
        "             trn_data_supplier=trn_data_supplier_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uQeN4dhPFOh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model2 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model3 = ZeeDenseNet(dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model4 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16})\n",
        "\n",
        "model5 = ZeeDenseNet(f_filter=16, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32})\n",
        "\n",
        "model6 = ZeeDenseNet(f_filter=32, dimensions_dict= {\"dimensions_to_sample\":(8,8)}, layers_filters={0:16, 1:32, 2:64})\n",
        "\n",
        "params_tune_grid = {\n",
        "    \n",
        "    \"model\": [model1, model2, model3, model4, model5, model6],\n",
        "    \n",
        "  \"epochs\":[30] ,\n",
        " \n",
        "  \"batch_size\" : [256],\n",
        "\n",
        "  \"max_lr\": [0.1, 0.4, 1, 10, 0.01],\n",
        "\n",
        "  \"min_lr\":[MIN_LEARNING_RATE],\n",
        "\n",
        "  \"end_anneal_pc\":[END_LR_SMOOTHING_PERC],\n",
        "\n",
        "  \"max_mom\":[MOMENTUM],\n",
        " \n",
        "  \"min_mom\":[MIN_MOMENTUM],\n",
        " \n",
        "  \"wd\":[WEIGHT_DECAY],\n",
        "  \n",
        "  \"skip_testing_epochs\":[0],\n",
        "    \n",
        "  \"batches_per_epoch\":[50000//BATCH_SIZE],\n",
        "    \n",
        "   \"comments\":[\"zeedense grid search\"]\n",
        "}\n",
        "\n",
        "\n",
        "obj.grid_search(params_tune_grid, trn_data_supplier, tst_data_supplier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV1dIBh3FOh4",
        "colab_type": "text"
      },
      "source": [
        "##### Viewing log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JEYtCUgPFOh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(\"../data/run_logger.csv\").tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LNpxFwWFOh8",
        "colab_type": "text"
      },
      "source": [
        "#### Numpy implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgrIs7rpFOh9",
        "colab_type": "raw"
      },
      "source": [
        "x_train, y_train, x_test, y_test = data_pipeline.load_saved_numpy_data(\n",
        "    train_path=[\"../data/train/train_x.npy\",\"../data/train/train_y.npy\"],\n",
        "    test_path=[\"../data/test/test_x.npy\", \"../data/test/test_y.npy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ptU07KFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global eval_dataset\n",
        "  \n",
        "    len_test = x_test[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test[:30,:,:,:],\n",
        "                                                   y_test[:30])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier(epoch_num):\n",
        "    \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "  \n",
        "    global y_train\n",
        "    \n",
        "    global x_train\n",
        "    \n",
        "    len_test = x_train[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_train[:50,:,:,:], \n",
        "                                        y_train[:50])).batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return (test_set, len_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCbZDfOFOh-",
        "colab_type": "raw"
      },
      "source": [
        "def tst_data_supplier_lr(epoch, test_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_test\n",
        "  \n",
        "    global y_test\n",
        "  \n",
        "    sample_idx = np.random.choice(y_test.size, test_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_test_lr = x_test[sample_idx, :, :, :]\n",
        "\n",
        "    y_test_lr = y_test[sample_idx]\n",
        "    \n",
        "    len_test = x_test_lr[0].shape[0]\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((x_test_lr,\n",
        "                                                   y_test_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (test_set, len_test)\n",
        "\n",
        "def trn_data_supplier_lr(epoch, train_num_batches=2):\n",
        " \n",
        "    batch_size = params_tune[\"batch_size\"]\n",
        "    \n",
        "    global x_train\n",
        "  \n",
        "    global y_train\n",
        "  \n",
        "    sample_idx = np.random.choice(y_train.size, train_num_batches * batch_size, replace=False)\n",
        "\n",
        "    x_train_lr = x_train[sample_idx, :, :, :]\n",
        "\n",
        "    y_train_lr = y_train[sample_idx]\n",
        "    \n",
        "    len_train = x_train_lr[0].shape[0]\n",
        "\n",
        "    train_set = tf.data.Dataset.from_tensor_slices((x_train_lr,\n",
        "                                                   y_train_lr)).batch(batch_size).prefetch(1)\n",
        "\n",
        "    return (train_set, len_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXJKLMH1XmnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}